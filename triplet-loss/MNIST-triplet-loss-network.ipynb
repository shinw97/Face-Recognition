{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# https://github.com/KinWaiCheuk/Triplet-net-keras/blob/master/Triplet%20NN%20Test%20on%20MNIST.ipynb\n",
    "from keras.layers import Input, Conv2D, Lambda, Dense, Flatten,MaxPooling2D, concatenate\n",
    "from keras.models import Model, Sequential\n",
    "from keras.regularizers import l2\n",
    "from keras import backend as K\n",
    "from keras.optimizers import SGD,Adam\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import sys\n",
    "from keras.backend import int_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_DIR = '../dataset/mnist-triplet-loss/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_print(op, tensors, message=None):\n",
    "    def print_message(x):\n",
    "        sys.stdout.write(message + \" %s\\n\" % x)\n",
    "        return x\n",
    "\n",
    "    prints = [tf.py_func(print_message, [tensor], tensor.dtype) for tensor in tensors]\n",
    "    with tf.control_dependencies(prints):\n",
    "        op = tf.identity(op)\n",
    "    return op\n",
    "\n",
    "# def triplet_loss(y_true, y_pred, alpha = 0.4):\n",
    "#     \"\"\"\n",
    "#     Implementation of the triplet loss function\n",
    "#     Arguments:\n",
    "#     y_true -- true labels, required when you define a loss in Keras, you don't need it in this function.\n",
    "#     y_pred -- python list containing three objects:\n",
    "#             anchor -- the encodings for the anchor data\n",
    "#             positive -- the encodings for the positive data (similar to anchor)\n",
    "#             negative -- the encodings for the negative data (different from anchor)\n",
    "#     Returns:\n",
    "#     loss -- real number, value of the loss\n",
    "#     \"\"\"\n",
    "# #     y_pred = tf_print(y_pred, [y_pred], message='y_pred ' + str(int_shape(y_true)))\n",
    "# #     print('y_pred.shape = ', y_pred)\n",
    "    \n",
    "#     total_length = y_pred.shape.as_list()[-1]\n",
    "    \n",
    "#     anchor = y_pred[:,0:int(total_length*1/3)]\n",
    "#     positive = y_pred[:,int(total_length*1/3):int(total_length*2/3)]\n",
    "#     negative = y_pred[:,int(total_length*2/3):int(total_length*3/3)]\n",
    "\n",
    "#     # distance between the anchor and the positive\n",
    "#     pos_dist = K.sum(K.square(anchor-positive),axis=1)\n",
    "\n",
    "#     # distance between the anchor and the negative\n",
    "#     neg_dist = K.sum(K.square(anchor-negative),axis=1)\n",
    "\n",
    "#     # compute loss\n",
    "#     basic_loss = pos_dist-neg_dist+alpha\n",
    "#     loss = K.maximum(basic_loss,0.0)\n",
    "    \n",
    "#     return loss\n",
    "\n",
    "def triplet_loss(y_true, y_pred, alpha = 0.2):\n",
    "    \"\"\"\n",
    "    Implementation of the triplet loss as defined by formula (3)\n",
    "    \n",
    "    Arguments:\n",
    "    y_true -- true labels, required when you define a loss in Keras, you don't need it in this function.\n",
    "    y_pred -- python list containing three objects:\n",
    "            anchor -- the encodings for the anchor images, of shape (None, 128)\n",
    "            positive -- the encodings for the positive images, of shape (None, 128)\n",
    "            negative -- the encodings for the negative images, of shape (None, 128)\n",
    "    \n",
    "    Returns:\n",
    "    loss -- real number, value of the loss\n",
    "    \"\"\"\n",
    "    \n",
    "    y_pred = tf_print(y_pred, [y_pred], message='y_pred ' + str(int_shape(y_pred)))\n",
    "    print('y_pred.shape = ', y_pred)\n",
    "    anchor, positive, negative = y_pred[:, 0], y_pred[:, 1], y_pred[:, 2]\n",
    "    \n",
    "    # Step 1: Compute the (encoding) distance between the anchor and the positive, you will need to sum over axis=-1\n",
    "    pos_dist = tf.reduce_sum(tf.square(tf.subtract(anchor,positive)),axis=-1) \n",
    "    # Step 2: Compute the (encoding) distance between the anchor and the negative, you will need to sum over axis=-1\n",
    "    neg_dist = tf.reduce_sum(tf.square(tf.subtract(anchor,negative)),axis=-1)\n",
    "    # Step 3: subtract the two previous distances and add alpha.\n",
    "    basic_loss = pos_dist - neg_dist + alpha\n",
    "    # Step 4: Take the maximum of basic_loss and 0.0. Sum over the training examples.\n",
    "    loss = tf.reduce_sum(tf.maximum(basic_loss , 0.0))\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def base_network(in_dims):\n",
    "    \"\"\"\n",
    "    Base network to be shared.\n",
    "    \"\"\"\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(5,(7,7),padding='same',input_shape=(in_dims[0],in_dims[1],in_dims[2],),activation='relu',name='conv1'))\n",
    "    model.add(MaxPooling2D((2,2),(2,2),padding='same',name='pool1'))\n",
    "    model.add(Conv2D(7,(5,5),padding='same',activation='relu',name='conv2'))\n",
    "    model.add(MaxPooling2D((2,2),(2,2),padding='same',name='pool2'))\n",
    "    model.add(Flatten(name='flatten'))\n",
    "    model.add(Dense(128,name='embeddings'))\n",
    "    model.add(Lambda(lambda x: tf.expand_dims(x, 1)))\n",
    "    # model.add(Dense(600))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_triplet_dataset():\n",
    "    X_train = np.load(DATASET_DIR + 'tr_triplets.npy')\n",
    "    X_test = np.load(DATASET_DIR + 'ts_triplets.npy')\n",
    "    return X_train, X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 28, 28, 1)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test = load_triplet_dataset()\n",
    "input_shape = X_train.shape[1:]\n",
    "input_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_y_train = np.empty((X_train.shape[0], 1))\n",
    "dummy_y_test = np.empty((X_test.shape[0], 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "anchor_input = Input((28,28,1, ), name='anchor_input')\n",
    "positive_input = Input((28,28,1, ), name='positive_input')\n",
    "negative_input = Input((28,28,1, ), name='negative_input')\n",
    "\n",
    "# Shared embedding layer for positive and negative items\n",
    "shared_network = base_network([28,28,1,])\n",
    "\n",
    "\n",
    "encoded_anchor = shared_network(anchor_input)\n",
    "encoded_positive = shared_network(positive_input)\n",
    "encoded_negative = shared_network(negative_input)\n",
    "\n",
    "\n",
    "merged_vector = concatenate([encoded_anchor, encoded_positive, encoded_negative], axis=1, name='merged_layer')\n",
    "\n",
    "\n",
    "adam = Adam(lr=0.0001, beta_1=0.9, beta_2=0.999)\n",
    "model = Model(inputs=[anchor_input,positive_input, negative_input], outputs=merged_vector)\n",
    "# model = Model(inputs=[anchor_input,positive_input, negative_input], outputs=[encoded_anchor, encoded_positive, encoded_negative])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_pred.shape =  Tensor(\"merged_layer_4/concat:0\", shape=(?, 3, 128), dtype=float32)\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "anchor_input (InputLayer)       (None, 28, 28, 1)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "positive_input (InputLayer)     (None, 28, 28, 1)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "negative_input (InputLayer)     (None, 28, 28, 1)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "sequential_5 (Sequential)       (None, 1, 128)       45164       anchor_input[0][0]               \n",
      "                                                                 positive_input[0][0]             \n",
      "                                                                 negative_input[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "merged_layer (Concatenate)      (None, 3, 128)       0           sequential_5[1][0]               \n",
      "                                                                 sequential_5[2][0]               \n",
      "                                                                 sequential_5[3][0]               \n",
      "==================================================================================================\n",
      "Total params: 45,164\n",
      "Trainable params: 45,164\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss=triplet_loss, optimizer=adam)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 54200 samples, validate on 8910 samples\n",
      "Epoch 1/10\n",
      "54200/54200 [==============================] - 12s 228us/step - loss: 0.0607 - val_loss: 1.1083\n",
      "Epoch 2/10\n",
      "54200/54200 [==============================] - 12s 225us/step - loss: 0.0546 - val_loss: 1.1358\n",
      "Epoch 3/10\n",
      "54200/54200 [==============================] - 12s 226us/step - loss: 0.0499 - val_loss: 1.1531\n",
      "Epoch 4/10\n",
      "54200/54200 [==============================] - 12s 229us/step - loss: 0.0430 - val_loss: 1.1450\n",
      "Epoch 5/10\n",
      "54200/54200 [==============================] - 12s 229us/step - loss: 0.0381 - val_loss: 1.1706\n",
      "Epoch 6/10\n",
      "54200/54200 [==============================] - 12s 230us/step - loss: 0.0305 - val_loss: 1.1854\n",
      "Epoch 7/10\n",
      "54200/54200 [==============================] - 13s 231us/step - loss: 0.0291 - val_loss: 1.2095\n",
      "Epoch 8/10\n",
      "54200/54200 [==============================] - 13s 232us/step - loss: 0.0227 - val_loss: 1.1724\n",
      "Epoch 9/10\n",
      "54200/54200 [==============================] - 13s 232us/step - loss: 0.0187 - val_loss: 1.2275\n",
      "Epoch 10/10\n",
      "54200/54200 [==============================] - 12s 230us/step - loss: 0.0164 - val_loss: 1.2248\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f47b6e6bb00>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit([X_train[:, 0] ,X_train[:, 1],X_train[:, 2]],\n",
    "          y=dummy_y_train, \n",
    "          validation_data=([X_test[:, 0], X_test[:, 1], X_test[:, 2]], dummy_y_test), \n",
    "          epochs=10, \n",
    "          batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('trained-weights-model/MNIST-triplet-loss-network-andrew-ng.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
